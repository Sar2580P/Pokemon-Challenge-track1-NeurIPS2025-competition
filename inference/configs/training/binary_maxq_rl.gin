import amago.agent
import inference.agent_tribe

#agent.Agent.reward_multiplier = 10.
#agent.Agent.tau = .004
#agent.Agent.online_coeff = 0.25
#agent.Agent.offline_coeff = 1.0
#agent.Agent.fbc_filter_func = @agent.binary_filter
#agent.Agent.num_actions_for_value_in_critic_loss = 1
#agent.Agent.num_actions_for_value_in_actor_loss = 1

agent_tribe.InferenceEntityMTA.reward_multiplier = 10.
agent_tribe.InferenceEntityMTA.tau = .004
agent_tribe.InferenceEntityMTA.online_coeff = 0.25
agent_tribe.InferenceEntityMTA.offline_coeff = 1.0
agent_tribe.InferenceEntityMTA.fbc_filter_func = @agent.binary_filter
agent_tribe.InferenceEntityMTA.num_actions_for_value_in_critic_loss = 3
agent_tribe.InferenceEntityMTA.num_actions_for_value_in_actor_loss = 3

MetamonAMAGOExperiment.l2_coeff = 1e-4
MetamonAMAGOExperiment.learning_rate = 1.5e-4
MetamonAMAGOExperiment.grad_clip = 1.5
MetamonAMAGOExperiment.critic_loss_weight = 10.
MetamonAMAGOExperiment.lr_warmup_steps = 1000