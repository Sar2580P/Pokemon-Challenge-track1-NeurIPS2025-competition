# ğŸ® Adaptive Regret Minimization for Multi-Task Agents via Reactive Hedge

<p align="center">
  <strong>Team: PAC-srsk-1729 | NeurIPS 2025 - PokÃ©mon Challenge Track 1</strong>
</p>

<br>

<div align="center">
    <img src="media/metamon_banner.png" alt="Metamon Banner" width="720">
</div>

<br>

<p align="center">
  <strong>ğŸ‘ï¸UğŸ‘ï¸</strong>
</p>


<p align="center">
  <a href="https://github.com/Sar2580P/Pokemon-Challenge-track1-NeurIPS2025-competition">
    <img src="https://img.shields.io/badge/GitHub-Repository-blue?style=for-the-badge&logo=github" alt="GitHub">
  </a>
  <a href="https://huggingface.co/jakegrigsby/metamon/tree/main">
    <img src="https://img.shields.io/badge/ğŸ¤—-Model_Checkpoints-yellow?style=for-the-badge" alt="HuggingFace">
  </a>
</p>

---

## ğŸ“‹ Abstract

We introduce a **"Tribe of Experts"** architecture for decision-making in multi-task reinforcement learning environments. To effectively ensemble a diverse population of agents (Entities) trained with varying hyperparameters, we propose a **Reactive Hedge** algorithm. This method minimizes cumulative regret during inference by dynamically weighting experts based on a normalized Temporal Difference (TD) error proxy. Furthermore, we implement an adaptive learning rate mechanism and a refined nucleus sampling strategy to balance exploration and stability.

---

## ğŸ† Competition Results

| Track Category | Rank | Notes |
|:---------------|:----:|:------|
| **Track 1 - Gen1ou** | ğŸ¥‰ **3rd** | High stability observed in expert ensemble |
| **Track 1 - Gen9ou** | **7th** | Impacted by team composition meta-game |

---

## ğŸš€ Quick Start

### Prerequisites

- **CUDA-capable GPU** (recommended)
- **Python 3.10**
- **Conda** (for environment management)
- **Node.js** (for PokÃ©mon Showdown server)

### 1. Clone the Repository

```bash
git clone https://github.com/Sar2580P/Pokemon-Challenge-track1-NeurIPS2025-competition.git
cd Pokemon-Challenge-track1
```

### 2. Create Environment

```bash
conda env create -f environment.yml
conda activate metamon
```

### 3. Set Environment Variables

```bash
export PYTHONPATH=$PYTHONPATH:.
export METAMON_CACHE_DIR="PAC-dataset"
```

### 4. Download Model Checkpoints

Model weights are available on HuggingFace: [jakegrigsby/metamon](https://huggingface.co/jakegrigsby/metamon/tree/main)

```bash
# Example: Download a specific checkpoint
modal volume get pokemon-showdown-gen1 results/HRM_Pokemon_Gen1/ckpts/latest/policy.pt model_weights.pt
```

---

## ğŸ“ Project Structure

```
Pokemon-Challenge-track1/
â”‚
â”œâ”€â”€ ğŸ“‚ custom/                      # Custom training components
â”‚   â”œâ”€â”€ ğŸ“‚ gen1/                    # Gen1-specific training scripts
â”‚   â”‚   â”œâ”€â”€ configs/                # Gin configuration files
â”‚   â”‚   â”œâ”€â”€ scripts/                # Shell scripts for training
â”‚   â”‚   â”œâ”€â”€ train.py                # Main training script
â”‚   â”‚   â”œâ”€â”€ train_opponent_modeling.py
â”‚   â”‚   â”œâ”€â”€ train_traj_encoder_KD.py
â”‚   â”‚   â”œâ”€â”€ train_vae_prior.py
â”‚   â”‚   â””â”€â”€ evaluate.py
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ gen9/                    # Gen9-specific training scripts
â”‚   â”‚   â”œâ”€â”€ configs/
â”‚   â”‚   â”œâ”€â”€ scripts/
â”‚   â”‚   â”œâ”€â”€ train.py
â”‚   â”‚   â”œâ”€â”€ train_opponent_modeling.py
â”‚   â”‚   â”œâ”€â”€ train_vae_prior.py
â”‚   â”‚   â””â”€â”€ evaluate.py
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ hrm_utils/               # HRM utility modules
â”‚   â”‚   â”œâ”€â”€ layers.py               # Custom attention & neural network layers
â”‚   â”‚   â”œâ”€â”€ common.py               # Common utilities
â”‚   â”‚   â”œâ”€â”€ losses.py               # Loss functions
â”‚   â”‚   â”œâ”€â”€ modules.py              # Neural network modules
â”‚   â”‚   â””â”€â”€ sparse_embedding.py     # Sparse embedding utilities
â”‚   â”‚
â”‚   â”œâ”€â”€ hrm_agent.py                # HRM Multi-Task Agent implementation
â”‚   â”œâ”€â”€ traj_encoder.py             # Trajectory Encoder (HRM-based)
â”‚   â”œâ”€â”€ experiment.py               # Custom experiment class
â”‚   â”œâ”€â”€ evaluate.py                 # Evaluation utilities
â”‚   â””â”€â”€ utils.py                    # General utilities
â”‚
â”œâ”€â”€ ğŸ“‚ inference/                   # Inference-time components
â”‚   â”œâ”€â”€ ğŸ“‚ configs/                 # Inference configurations
â”‚   â”‚   â”œâ”€â”€ models/                 # Model architecture configs
â”‚   â”‚   â””â”€â”€ training/               # Training hyperparameter configs
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ play_battle/             # Battle playing scripts
â”‚   â”‚   â””â”€â”€ gen1.py, gen9.py
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ scripts/                 # Evaluation shell scripts
â”‚   â”‚   â”œâ”€â”€ eval_gen1.sh
â”‚   â”‚   â””â”€â”€ eval_gen9.sh
â”‚   â”‚
â”‚   â”œâ”€â”€ agent_tribe.py              # ğŸŒŸ Tribe of Experts Implementation
â”‚   â”œâ”€â”€ population_config.py        # Expert population configuration
â”‚   â”œâ”€â”€ experiment_tribe.py         # Tribe experiment manager
â”‚   â””â”€â”€ evaluate.py                 # Inference evaluation
â”‚
â”œâ”€â”€ ğŸ“‚ metamon/                     # Core metamon library (forked)
â”œâ”€â”€ ğŸ“‚ amago/                       # AMAGO framework (forked)
â”œâ”€â”€ ğŸ“‚ server/                      # PokÃ©mon Showdown server
â”œâ”€â”€ ğŸ“‚ team_design/                 # Team composition analysis
â”‚
â”œâ”€â”€ environment.yml                 # Conda environment specification
â”œâ”€â”€ pyproject.toml                  # Python project configuration
â””â”€â”€ pokemon_writeup.pdf             # Competition report
```

---

## ğŸ§  Architecture Overview

### The Reactive Hedge Algorithm

The core mechanism maintains a probability distribution (weights) over the expert population. At each time step, expert weights are updated based on TD-error:

```
w_{i,t+1} = (w_{i,t} Â· exp(-Î·_{i,t} Â· L_{i,t})) / Î£_j(w_{j,t} Â· exp(-Î·_{j,t} Â· L_{j,t}))
```

### Loss Design: Normalized TD-Error Proxy

```python
Î´_i,t = (r_t + Î³_i Â· V_i(s_{t+1})) - V_i(s_t)   # TD Error
z_i,t = Î´_i,t / (Ïƒ_popart + Îµ)                   # Normalize
L_i,t = -tanh(z_i,t)                             # Proxy Loss
```

### Dynamic Ensembling Pipeline

1. **Per-Gamma Refinement**: Each expert refines its policy using internal advantage estimates
2. **Top-K Masking**: Filters poor-performing experts
3. **Nucleus Sampling**: Final action selection via Top-p sampling

---

## ğŸ‹ï¸ Training

### Standard Training (Gen1)

```bash
cd custom/gen1
bash scripts/train.sh
```

### Training with Opponent Modeling

```bash
python custom/gen1/train_opponent_modeling.py
```

### Knowledge Distillation for Trajectory Encoder

```bash
python custom/gen1/train_traj_encoder_KD.py
```

### VAE Prior Training

```bash
python custom/gen1/train_vae_prior.py
```

### Configuration (Gin)

Training configurations use Google's Gin library. Key config files:

| Config Type | Location |
|-------------|----------|
| Model Architecture | `custom/gen1/configs/` or `custom/gen9/configs/` |
| Training Hyperparameters | `inference/configs/training/` |
| Agent Configuration | `inference/configs/models/` |

---

## ğŸ¯ Inference & Evaluation

### Running Evaluation (Gen1)

```bash
bash inference/scripts/eval_gen1.sh
```

### Running Evaluation (Gen9)

```bash
bash inference/scripts/eval_gen9.sh
```

### Evaluation Options

Edit the shell scripts to configure:

| Parameter | Description | Options |
|-----------|-------------|---------|
| `EVAL_TYPE` | Opponent type | `heuristic`, `il`, `ladder`, `pokeagent` |
| `TOTAL_BATTLES` | Number of battles | Integer |
| `TEAM_SET` | Team composition | `competitive` |
| `BATTLE_BACKEND` | Backend system | `metamon`, `poke-env` |

### Expert Population Configuration

Configure the expert ensemble in `inference/population_config.py`:

```python
GEN1_Models = [
    ("SyntheticRLV2", {'checkpoint': 40, 'model_gin_config': ..., 'train_gin_config': ...}),
    ("SyntheticRLV2", {'checkpoint': 46, ...}),
    # Add more experts...
]
```

---

## ğŸ”§ Key Components

### `custom/hrm_agent.py`

The **HRM_MultiTaskAgent** class implements:
- Multi-gamma policy learning
- Advantage-weighted policy refinement
- Nucleus sampling for action selection
- Component-wise checkpoint initialization

### `custom/traj_encoder.py`

The **HRMTrajEncoder** implements a Hierarchical Reasoning Model with:
- Chunked attention for long sequences
- Temporal summary aggregation
- VAE-based opponent modeling (optional)

### `inference/agent_tribe.py`

The **InferenceTribeMTA** class implements:
- Reactive Hedge weight updates
- RMS-Prop style adaptive learning rates
- Top-K expert filtering
- Dynamic policy ensembling

---

## ğŸ“Š Evaluation Metrics

The framework tracks several diagnostic metrics:

| Metric | Description |
|--------|-------------|
| **Policy Entropy (H_Ï€)** | Uncertainty of the ensembled policy |
| **Weight Entropy (H_w)** | Expert diversity / democratic ensemble indicator |
| **Confidence Margin** | Probability gap between top two actions |
| **TD Error** | Temporal difference error for weight updates |

---

## ğŸ› ï¸ Modal Deployment (Cloud)

For cloud-based training/evaluation using Modal:

```bash
modal shell --volume my-volume
```

---

## ğŸ“š Dependencies

Key dependencies from `environment.yml`:

| Package | Version | Purpose |
|---------|---------|---------|
| `torch` | CUDA 12.x | Deep Learning |
| `gymnasium` | 0.29.1 | RL environments |
| `einops` | 0.8.1 | Tensor operations |
| `gin-config` | 0.5.0 | Configuration |
| `wandb` | 0.21.1 | Experiment tracking |
| `accelerate` | 1.10.0 | Distributed training |

---

## ğŸ“– References

1. **Van Erven, T., et al.** (2011). *Adaptive hedge*. NeurIPS.
2. **Freund, Y., & Schapire, R. E.** (1997). *A decision-theoretic generalization of on-line learning*. JCSS.
3. **Ren, Y., et al.** (2024). *HRM: Hierarchical Reasoning Model*. ICLR.

---

## ğŸ™ Acknowledgments

Special thanks to:
- **Competition Organizers** for hosting this challenging event
- **Jake Grigsby** for detailed guidance and support throughout the competition
- The **metamon** and **AMAGO** framework developers

---

## ğŸ“„ License

This project is licensed under the terms specified in the [LICENSE](LICENSE) file.

---
